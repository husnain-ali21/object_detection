{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanilla Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCsOsa8tAlc0t314Vzhme+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/husnain-ali21/transformers/blob/main/vanilla_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICK7sbXxHyD"
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import einops\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zMcBq80YMkr"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self , heads , embedding_dim , sequence_size , batch_dim ):\n",
        "        super(SelfAttention ,self).__init__() \n",
        "        self.heads = heads \n",
        "        self.embedding_dim = embedding_dim \n",
        "        self.sequence_size = sequence_size\n",
        "        self.batch_dim = batch_dim\n",
        "\n",
        "        self.transform_to_keys = nn.Linear(embedding_dim , embedding_dim * heads)\n",
        "        self.transform_to_queries = nn.Linear(embedding_dim , embedding_dim*heads)\n",
        "        self.transform_to_values = nn.Linear(embedding_dim , embedding_dim * heads)\n",
        "\n",
        "        self.final = nn.Linear(heads*embedding_dim  , embedding_dim)\n",
        "\n",
        "        def forward(self , input ,  mask = None):\n",
        "            assert input.shape == (self.batch_dim ,self.sequence_size , self.embedding_dim) , \"Input shape is not (b x s x e )\"\n",
        "            queries = einops.rearrange(self.transform_to_queries(input) , \\\n",
        "                                       'batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            keys = einops.rearrange(self.transform_to_keys(input) , \\\n",
        "                                   ' batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            values = einops.rearrange(self.transform_to_values(input) ,\\\n",
        "                                      'batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            dot_product = torch.einsum(\"bse,bte->bst\" , [keys ,queries]) / np.sqrt(self.embedding_dim)\n",
        "            if mask is not None :\n",
        "                dot_product = dot_product.masked_fill(mask[None] , -np.inf)\n",
        "            attention_weights = torch.nn.functional.softmax(dot_product , dim =-1)\n",
        "            attention_applied_output = torch.einsum('bst,bte->bse',[attention_weights , values]) # b here actually is b*h \n",
        "\n",
        "            return self.final(einops.rearrange(attention_applied_output ,\\\n",
        "                                               \"(batch heads) seq_size emb_size ->  batch seq_size (heads emb_dim)\" , heads = self.heads))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMatgurjjfQY"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self  ,ffnn_dim , heads , embedding_dim , sequence_size , batch_dim):\n",
        "        super(TransformerBlock , self).__init__() \n",
        "        self.heads = heads \n",
        "        self.embedding_dim = embedding_dim \n",
        "        self.sequence_size = sequence_size\n",
        "        self.batch_dim = batch_dim\n",
        "\n",
        "        self.norm1 = nn.LayerNorm()\n",
        "        self.norm2 = nn.LayerNorm()\n",
        "\n",
        "        self.MHA = MultiHeadAttention(self.heads , self.embedding_dim , self.sequence_size ,self.batch_dim)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim , ffnn_dim) ,nn.LeakyReLU(0.3) , nn.Dropout(dropout) , \n",
        "            nn.Linear(ffnn ,embedding_dim) ,nn.LeakyReLU(0.3) , nn.Dropout(dropout) \n",
        "        )\n",
        "        def forward(self , input , mask = None):\n",
        "            output = self.norm1(self.drop(self.MHA(input , mask )) + input)\n",
        "            return self.norm2(self.ffnn(output) + output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbrcYQt0u4j2"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self ,n_blocks , ffnn_dim , heads , embedding_dim , sequence_size , batch_dim)):\n",
        "        super(Transformer , self).__init__()\n",
        "        self.blocks = [TransformerBlock(ffnn_dim , heads , embedding_dim , sequence_size , batch_dim) for _ in range(n_blocks))]\n",
        "        self.layers = nn.Sequential(self.blocks)\n",
        "\n",
        "        def forward(self , input , mask) = None:\n",
        "            return self.layers(input)\n",
        "\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}