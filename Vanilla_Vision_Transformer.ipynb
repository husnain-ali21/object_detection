{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla Vision Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Rm65jHnrcWWN3VHb3yke6hjBDZyPWJDe",
      "authorship_tag": "ABX9TyNyu1z4mzpGheivoYSZ49F7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/husnain-ali21/transformers/blob/main/Vanilla_Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkJWd7iwVV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc14202f-3608-4d82-efea-64912ae3dddf"
      },
      "source": [
        "%cd ./drive/MyDrive/Colab\\ Notebooks\n",
        "\n",
        "import torch \n",
        "import numpy as np \n",
        "from einops import rearrange\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB_3FT19zy8q"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self , heads , embedding_dim , sequence_size , batch_dim ):\n",
        "        super(SelfAttention ,self).__init__() \n",
        "        self.heads = heads \n",
        "        self.embedding_dim = embedding_dim \n",
        "        self.sequence_size = sequence_size\n",
        "        self.batch_dim = batch_dim\n",
        "\n",
        "        self.transform_to_keys = nn.Linear(embedding_dim , embedding_dim * heads)\n",
        "        self.transform_to_queries = nn.Linear(embedding_dim , embedding_dim*heads)\n",
        "        self.transform_to_values = nn.Linear(embedding_dim , embedding_dim * heads)\n",
        "\n",
        "        self.final = nn.Linear(heads*embedding_dim  , embedding_dim)\n",
        "\n",
        "        def forward(self , input ,  mask = None):\n",
        "            assert input.shape == (self.batch_dim ,self.sequence_size , self.embedding_dim) , \"Input shape is not (b x s x e )\"\n",
        "            queries = einops.rearrange(self.transform_to_queries(input) , \\\n",
        "                                       'batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            keys = einops.rearrange(self.transform_to_keys(input) , \\\n",
        "                                   ' batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            values = einops.rearrange(self.transform_to_values(input) ,\\\n",
        "                                      'batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            dot_product = torch.einsum(\"bse,bte->bst\" , [keys ,queries]) / np.sqrt(self.embedding_dim)\n",
        "            if mask is not None :\n",
        "                dot_product = dot_product.masked_fill(mask[None] , -np.inf)\n",
        "            attention_weights = torch.nn.functional.softmax(dot_product , dim =-1)\n",
        "            attention_applied_output = torch.einsum('bst,bte->bse',[attention_weights , values]) # b here actually is b*h \n",
        "\n",
        "            return self.final(einops.rearrange(attention_applied_output ,\\\n",
        "                                               \"(batch heads) seq_size emb_size ->  batch seq_size (heads emb_dim)\" , heads = self.heads))\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1lYR1-dz26d"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self  ,ffnn_dim , heads , embedding_dim , sequence_size , batch_dim):\n",
        "        super(TransformerBlock , self).__init__() \n",
        "        self.heads = heads \n",
        "        self.embedding_dim = embedding_dim \n",
        "        self.sequence_size = sequence_size\n",
        "        self.batch_dim = batch_dim\n",
        "\n",
        "        self.norm1 = nn.LayerNorm()\n",
        "        self.norm2 = nn.LayerNorm()\n",
        "\n",
        "        self.MHA = MultiHeadAttention(self.heads , self.embedding_dim , self.sequence_size ,self.batch_dim)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim , ffnn_dim) ,nn.LeakyReLU(0.3) , nn.Dropout(dropout) , \n",
        "            nn.Linear(ffnn ,embedding_dim) ,nn.LeakyReLU(0.3) , nn.Dropout(dropout) \n",
        "        )\n",
        "        def forward(self , input , mask = None):\n",
        "            output = self.norm1(self.drop(self.MHA(input , mask )) + input)\n",
        "            return self.norm2(self.ffnn(output) + output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_JvHFGvz8Ut"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self ,n_blocks , ffnn_dim , heads , embedding_dim , sequence_size , batch_dim)):\n",
        "        super(Transformer , self).__init__()\n",
        "        self.blocks = [TransformerBlock(ffnn_dim , heads , embedding_dim , sequence_size , batch_dim) for _ in range(n_blocks))]\n",
        "        self.layers = nn.Sequential(self.blocks)\n",
        "\n",
        "        def forward(self , input , mask) = None:\n",
        "            return self.layers(input)\n",
        "\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR3Kcz3DwZuX"
      },
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self , image_dim , channels_in=3 , patch_size=16 , patch_dim , blocks=6 , heads=4 , linear_block_dim=1024,\n",
        "                 drop = 0.2  ,p_dim):\n",
        "        \"\"\"\n",
        "        Parameters .\n",
        "\n",
        "        image_dim: image_size C x H x W\n",
        "        channels_in: no of channels of image input (C)\n",
        "        patch_size: patch dim to divide image in\n",
        "        patch_dim: dim of linear layer to project patches in a low dimensional space\n",
        "        blocks: no of transformer encoder blocks\n",
        "        heads: not of Multi Head Attention Heads\n",
        "        linear_block_dim: transformer's linear block's dim\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        super(VisionTransformer,self)__init__()\n",
        "        assert image_dim % patch_size == 0 ,f\"Patch size {patch_size} is not divisible.\"\n",
        "\n",
        "        self.patch_dim = patched_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.blocks = block\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji877WzSyZDh",
        "outputId": "e44923eb-1c63-4845-c18a-9a577e30df91"
      },
      "source": [
        "print(patched_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXheVvDBxOis"
      },
      "source": [
        "from einops import rearrange\n",
        "p = 16 # P in maths\n",
        "x_p = rearrange(t1, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElfvmYmryMXO",
        "outputId": "fbdb5432-6c80-4f6b-a36b-deeb1de5a340"
      },
      "source": [
        "print(x_p.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avNNKWscyt1j",
        "outputId": "e1205a25-7710-4afe-e626-c2259918e04d"
      },
      "source": [
        "(32*32 )/(16**2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFI6x-3PzHld"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}