{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla Vision Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Rm65jHnrcWWN3VHb3yke6hjBDZyPWJDe",
      "authorship_tag": "ABX9TyNebR0x/inDRr3F0RPdpSQ8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkJWd7iwVV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc14202f-3608-4d82-efea-64912ae3dddf"
      },
      "source": [
        "%cd ./drive/MyDrive/Colab\\ Notebooks\n",
        "\n",
        "import torch \n",
        "import numpy as np \n",
        "from einops import rearrange\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB_3FT19zy8q"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self , heads , embedding_dim , sequence_size , batch_dim ):\n",
        "        super(SelfAttention ,self).__init__() \n",
        "        self.heads = heads \n",
        "        self.embedding_dim = embedding_dim \n",
        "        self.sequence_size = sequence_size\n",
        "        self.batch_dim = batch_dim\n",
        "\n",
        "        self.transform_to_keys = nn.Linear(embedding_dim , embedding_dim * heads)\n",
        "        self.transform_to_queries = nn.Linear(embedding_dim , embedding_dim*heads)\n",
        "        self.transform_to_values = nn.Linear(embedding_dim , embedding_dim * heads)\n",
        "\n",
        "        self.final = nn.Linear(heads*embedding_dim  , embedding_dim)\n",
        "\n",
        "        def forward(self , input ,  mask = None):\n",
        "            assert input.shape == (self.batch_dim ,self.sequence_size , self.embedding_dim) , \"Input shape is not (b x s x e )\"\n",
        "            queries = einops.rearrange(self.transform_to_queries(input) , \\\n",
        "                                       'batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            keys = einops.rearrange(self.transform_to_keys(input) , \\\n",
        "                                   ' batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            values = einops.rearrange(self.transform_to_values(input) ,\\\n",
        "                                      'batch seq_size (heads emb_dim) -> (batch heads) seq_size emb_dim ' , heads = self.heads)\n",
        "            dot_product = torch.einsum(\"bse,bte->bst\" , [keys ,queries]) / np.sqrt(self.embedding_dim)\n",
        "            if mask is not None :\n",
        "                dot_product = dot_product.masked_fill(mask[None] , -np.inf)\n",
        "            attention_weights = torch.nn.functional.softmax(dot_product , dim =-1)\n",
        "            attention_applied_output = torch.einsum('bst,bte->bse',[attention_weights , values]) # b here actually is b*h \n",
        "\n",
        "            return self.final(einops.rearrange(attention_applied_output ,\\\n",
        "                                               \"(batch heads) seq_size emb_size ->  batch seq_size (heads emb_dim)\" , heads = self.heads))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1lYR1-dz26d"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self  ,ffnn_dim , heads , embedding_dim , sequence_size , batch_dim):\n",
        "        super(TransformerBlock , self).__init__() \n",
        "        self.heads = heads \n",
        "        self.embedding_dim = embedding_dim \n",
        "        self.sequence_size = sequence_size\n",
        "        self.batch_dim = batch_dim\n",
        "\n",
        "        self.norm1 = nn.LayerNorm()\n",
        "        self.norm2 = nn.LayerNorm()\n",
        "\n",
        "        self.MHA = MultiHeadAttention(self.heads , self.embedding_dim , self.sequence_size ,self.batch_dim)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.ffnn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim , ffnn_dim) ,nn.LeakyReLU(0.3) , nn.Dropout(dropout) , \n",
        "            nn.Linear(ffnn ,embedding_dim) ,nn.LeakyReLU(0.3) , nn.Dropout(dropout) \n",
        "        )\n",
        "        def forward(self , input , mask = None):\n",
        "            output = self.norm1(self.drop(self.MHA(input , mask )) + input)\n",
        "            return self.norm2(self.ffnn(output) + output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_JvHFGvz8Ut"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self ,n_blocks , ffnn_dim , heads , embedding_dim , sequence_size , batch_dim)):\n",
        "        super(Transformer , self).__init__()\n",
        "        self.blocks = [TransformerBlock(ffnn_dim , heads , embedding_dim , sequence_size , batch_dim) for _ in range(n_blocks))]\n",
        "        self.layers = nn.Sequential(self.blocks)\n",
        "\n",
        "        def forward(self , input , mask) = None:\n",
        "            return self.layers(input)\n",
        "\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR3Kcz3DwZuX"
      },
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self , image_dim   ,batch_dim ,num_classes =  10, channels_in=3 , p_dim = 512, patch_size=16 , \\\n",
        "                  blocks=6 , heads=4 , linear_block_dim=1024,classification = True  , \\\n",
        "                 drop = 0.2  , transformer_enc = None):\n",
        "        \"\"\"\n",
        "        Parameters .\n",
        "\n",
        "        image_dim: image_size C x H x W\n",
        "        num_classes: classification task classes\n",
        "        channels_in: no of channels of image input (C)\n",
        "        batch_dim : for Transformer encoder\n",
        "        p_dim: the linear layer's dim to project the patches for Multi Head Self Attention\n",
        "        patch_size: patch dim to divide image in\n",
        "        blocks: no of transformer encoder blocks\n",
        "        heads: not of Multi Head Attention Heads\n",
        "        linear_block_dim: transformer's linear block's dim\n",
        "        classification : for an extra CLS token\n",
        "        drop : probobility to drop in dropout layers\n",
        "        transformer_enc  : transformer encoder block provided\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        super(VisionTransformer,self)__init__()\n",
        "        assert image_dim % patch_size == 0 ,f\"Patch size {patch_size} is not divisible.\"\n",
        "\n",
        "        self.patch_dim = patched_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.blocks = blocks\n",
        "        self.classify = classification\n",
        "        N_patches = (image_dim // self.patch_size)**2\n",
        "        self.token_in = in_channels *((self.patch_size)**2) # P2C -->Linear projection--> p_dim\n",
        "        self.p_dim = p_dim\n",
        "\n",
        "        self.Project_Patches = nn.Linear(self.token_in , self.p_dim)\n",
        "        self.Embedding_dropout = nn.Dropout(drop)\n",
        "\n",
        "        if self.classify:\n",
        "            self.cls = nn.parameter(torch.randn(1,1,self.p_dim))\n",
        "            self.positinoal_embeddings = nn.Parameter(torch.randn(tokens+1 , self.p_dim))\n",
        "            self.top_head = nn.Linear(self.p_dim ,num_classes)\n",
        "        else : \n",
        "            self.positinoal_embeddings = nn.Parameter(torch.randn(tokens , self.p_dim))\n",
        "\n",
        "\n",
        "        if transformer_enc is None:\n",
        "            self.Transformer_encoder = TransformerEncoder(blocks , p_dim , heads , linear_block_dim , N_patches  ,batch_dim)\n",
        "\n",
        "\n",
        "    `   else :\n",
        "            self.Transformer_encoder = transformer_enc\n",
        "\n",
        "    def expand_cls_to_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            batch: batch size\n",
        "        Returns: \n",
        "            cls token expanded to the batch size\n",
        "        \"\"\"\n",
        "        return self.cls_token.expand([batch, -1, -1])\n",
        "\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        batch_size = img.shape[0]\n",
        "        img_patches = rearrange(\n",
        "            img, 'b c (p_1 x) (p_2 y) -> b (x y) (p_1 p_2 c)',\n",
        "                                p_1=self.patch_size, p_2=self.patch_size)\n",
        "        \n",
        "        # Linear projection of patches with linear layer\n",
        "        #  +  add pos emb\n",
        "\n",
        "        img_patches = self.Project_Patches(img_patches)\n",
        "        if self.classification:\n",
        "            img_patches = torch.cat(\n",
        "                (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n",
        "        patch_embeddings = self.Embedding_dropout(img_patches + self.positional_embeddings)\n",
        "\n",
        "        # feed patch_embeddings and output of transformer , shape: [batch, tokens, dim]\n",
        "\n",
        "        y = self.Transformer_encoder(patch_embeddings, mask)\n",
        "        if self.classification:\n",
        "            # we index only the cls token for classification\n",
        "            return self.top_head(y[:, 0, :])\n",
        "        else:\n",
        "            return y"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}